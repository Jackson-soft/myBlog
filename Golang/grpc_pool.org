#+TITLE: /grpc pool/ 设计
* 客户端连接流程
* 服务发现 *Resolver*
** /Builder/
#+BEGIN_SRC go
  // Builder creates a resolver that will be used to watch name resolution updates.
  type Builder interface {
      // Build creates a new resolver for the given target.
      //
      // gRPC dial calls Build synchronously, and fails if the returned error is
      // not nil.
      Build(target Target, cc ClientConn, opts BuildOptions) (Resolver, error)
      // Scheme returns the scheme supported by this resolver.
      // Scheme is defined at https://github.com/grpc/grpc/blob/master/doc/naming.md.
      Scheme() string
  }
#+END_SRC
向grpc注册服务发现实现时，实际上注册的是Builder
** /Resolver/
#+BEGIN_SRC go
  // Resolver watches for the updates on the specified target.
  // Updates include address updates and service config updates.
  type Resolver interface {
      // ResolveNow will be called by gRPC to try to resolve the target name
      // again. It's just a hint, resolver can ignore this if it's not necessary.
      //
      // It could be called multiple times concurrently.
      // 当有连接被出现异常时，会触发该方法
      ResolveNow(ResolveNowOptions)
      // Close closes the resolver.
      Close()
  }
#+END_SRC
** /ClientConn/
#+BEGIN_SRC go
  // ClientConn contains the callbacks for resolver to notify any updates
  // to the gRPC ClientConn.
  //
  // This interface is to be implemented by gRPC. Users should not need a
  // brand new implementation of this interface. For the situations like
  // testing, the new implementation should embed this interface. This allows
  // gRPC to add new methods to this interface.
  type ClientConn interface {
      // UpdateState updates the state of the ClientConn appropriately.
      // 服务列表和服务配置更新回调接口
      UpdateState(State)
      // ReportError notifies the ClientConn that the Resolver encountered an
      // error.  The ClientConn will notify the load balancer and begin calling
      // ResolveNow on the Resolver with exponential backoff.
      ReportError(error)
      // NewAddress is called by resolver to notify ClientConn a new list
      // of resolved addresses.
      // The address list should be the complete list of resolved addresses.
      //
      // Deprecated: Use UpdateState instead.
      NewAddress(addresses []Address)
      // NewServiceConfig is called by resolver to notify ClientConn a new
      // service config. The service config should be provided as a json string.
      //
      // Deprecated: Use UpdateState instead.
      NewServiceConfig(serviceConfig string)
      // ParseServiceConfig parses the provided service config and returns an
      // object that provides the parsed config.
      ParseServiceConfig(serviceConfigJSON string) *serviceconfig.ParseResult
  }

#+END_SRC
其中Builder接口用来创建Resolver，然后将其注册到grpc中，其中通过scheme来标识，而Resolver
接口则是提供服务发现功能。当resover发现服务列表发生变更时，会通过ClientConn回调接口通知上层。

/grpc/ 也为我们提供了注册接口：
#+BEGIN_SRC go
  var (
      // m is a map from scheme to resolver builder.
      m = make(map[string]Builder)
      // defaultScheme is the default scheme to use.
      defaultScheme = "passthrough"
  )

  // Register registers the resolver builder to the resolver map. b.Scheme will be
  // used as the scheme registered with this builder.
  func Register(b Builder) {
      m[b.Scheme()] = b
  }

  // Get returns the resolver builder registered with the given scheme.
  //
  // If no builder is register with the scheme, nil will be returned.
  func Get(scheme string) Builder {
      if b, ok := m[scheme]; ok {
          return b
      }
      return nil
  }
#+END_SRC
如果没有提供自定义的，默认的就是 /passthrough/ 。
** 创建 /Resolver/
在我们调用 /grpc.Dial/ 的时候会解析传过来的 /target/ 然后创建对应的 /resolver/ 。
#+BEGIN_SRC go
  func DialContext(ctx context.Context, target string, opts ...DialOption) (conn *ClientConn, err error) {
      cc := &ClientConn{
          target:            target,
          csMgr:             &connectivityStateManager{},
          conns:             make(map[*addrConn]struct{}),
          dopts:             defaultDialOptions(),
          blockingpicker:    newPickerWrapper(),
          czData:            new(channelzData),
          firstResolveEvent: grpcsync.NewEvent(),
      }
      /*
         ......
       ,*/

      if cc.dopts.resolverBuilder == nil {
          // Only try to parse target when resolver builder is not already set.
          // 解析target，根据target的scheme获取对应的resolver
          // target 的格式 scheme://authority/endpoint
          cc.parsedTarget = parseTarget(cc.target)
          grpclog.Infof("parsed scheme: %q", cc.parsedTarget.Scheme)
          cc.dopts.resolverBuilder = resolver.Get(cc.parsedTarget.Scheme)
          if cc.dopts.resolverBuilder == nil {
              // If resolver builder is still nil, the parsed target's scheme is
              // not registered. Fallback to default resolver and set Endpoint to
              // the original target.
              grpclog.Infof("scheme %q not registered, fallback to default scheme", cc.parsedTarget.Scheme)
              cc.parsedTarget = resolver.Target{
                  Scheme:   resolver.GetDefaultScheme(), // 默认的 passthrough
                  Endpoint: target,// 这时候参数target就是endpoint，passthrough的实现就是直接返回endpoint，即不使用服务发现功能，参数Dial传进来的地址就是grpc server的地址
              }
              cc.dopts.resolverBuilder = resolver.Get(cc.parsedTarget.Scheme)
          }
      } else {
           // 如果Dial的option中手动指定了需要使用的resolver，那么endpoint也是target
          cc.parsedTarget = resolver.Target{Endpoint: target}
      }
      // ....
      // Build the resolver.
      // newCCResolverWrapper方法内调用builder的Build接口创建resolver
      rWrapper, err := newCCResolverWrapper(cc)
      if err != nil {
          return nil, fmt.Errorf("failed to build resolver: %v", err)
      }
      // ....
      return cc, nil
  }
#+END_SRC
来看 /newCCResolverWrapper/
#+BEGIN_SRC go
  // newCCResolverWrapper uses the resolver.Builder stored in the ClientConn to
  // build a Resolver and returns a ccResolverWrapper object which wraps the
  // newly built resolver.
  func newCCResolverWrapper(cc *ClientConn) (*ccResolverWrapper, error) {
      // 在DialContext方法中，已经初始化了resolverBuilder
      rb := cc.dopts.resolverBuilder
      if rb == nil {
          return nil, fmt.Errorf("could not get resolver for scheme: %q", cc.parsedTarget.Scheme)
      }

      // ccResolverWrapper实现resolver.ClientConn接口，用于提供服务列表变更之后的通知回调接口
      ccr := &ccResolverWrapper{
          cc:   cc,
          done: grpcsync.NewEvent(),
      }

      var credsClone credentials.TransportCredentials
      if creds := cc.dopts.copts.TransportCredentials; creds != nil {
          credsClone = creds.Clone()
      }
      rbo := resolver.BuildOptions{
          DisableServiceConfig: cc.dopts.disableServiceConfig,
          DialCreds:            credsClone,
          CredsBundle:          cc.dopts.copts.CredsBundle,
          Dialer:               cc.dopts.copts.Dialer,
      }

      var err error
      // We need to hold the lock here while we assign to the ccr.resolver field
      // to guard against a data race caused by the following code path,
      // rb.Build-->ccr.ReportError-->ccr.poll-->ccr.resolveNow, would end up
      // accessing ccr.resolver which is being assigned here.
      ccr.resolverMu.Lock()
      defer ccr.resolverMu.Unlock()
      // 创建resovler
      // 就是 passthroughResolver
      ccr.resolver, err = rb.Build(cc.parsedTarget, ccr, rbo)
      if err != nil {
          return nil, err
      }
      return ccr, nil
  }

#+END_SRC
/passthroughResolver/ 的注册：
#+BEGIN_SRC go
  func (*passthroughBuilder) Build(target resolver.Target, cc resolver.ClientConn, opts resolver.BuildOptions) (resolver.Resolver, error) {
      r := &passthroughResolver{
          target: target,
          cc:     cc,
      }
      // 执行服务发现逻辑
      r.start()
      return r, nil
  }

  func (r *passthroughResolver) start() {
      // 调用 resolver 的 ClientConn 的 UpdateState 接口通知上层
      r.cc.UpdateState(resolver.State{Addresses: []resolver.Address{{Addr: r.target.Endpoint}}})
  }

#+END_SRC
* 负载均衡 *Balancer*
** 接口
+ /Balancer/
#+BEGIN_SRC go
  // Balancer takes input from gRPC, manages SubConns, and collects and aggregates
  // the connectivity states.
  //
  // It also generates and updates the Picker used by gRPC to pick SubConns for RPCs.
  //
  // HandleSubConnectionStateChange, HandleResolvedAddrs and Close are guaranteed
  // to be called synchronously from the same goroutine.
  // There's no guarantee on picker.Pick, it may be called anytime.
  type Balancer interface {
      // HandleSubConnStateChange is called by gRPC when the connectivity state
      // of sc has changed.
      // Balancer is expected to aggregate all the state of SubConn and report
      // that back to gRPC.
      // Balancer should also generate and update Pickers when its internal state has
      // been changed by the new state.
      //
      // Deprecated: if V2Balancer is implemented by the Balancer,
      // UpdateSubConnState will be called instead.
      HandleSubConnStateChange(sc SubConn, state connectivity.State)
      // HandleResolvedAddrs is called by gRPC to send updated resolved addresses to
      // balancers.
      // Balancer can create new SubConn or remove SubConn with the addresses.
      // An empty address slice and a non-nil error will be passed if the resolver returns
      // non-nil error to gRPC.
      //
      // Deprecated: if V2Balancer is implemented by the Balancer,
      // UpdateClientConnState will be called instead.
      HandleResolvedAddrs([]resolver.Address, error)
      // Close closes the balancer. The balancer is not required to call
      // ClientConn.RemoveSubConn for its existing SubConns.
      Close()
  }

#+END_SRC
+ /ClientConn/ 回调接口
#+BEGIN_SRC go
  // ClientConn represents a gRPC ClientConn.
  //
  // This interface is to be implemented by gRPC. Users should not need a
  // brand new implementation of this interface. For the situations like
  // testing, the new implementation should embed this interface. This allows
  // gRPC to add new methods to this interface.
  type ClientConn interface {
      // NewSubConn is called by balancer to create a new SubConn.
      // It doesn't block and wait for the connections to be established.
      // Behaviors of the SubConn can be controlled by options.
      // 根据地址创建网络连接
      NewSubConn([]resolver.Address, NewSubConnOptions) (SubConn, error)
      // RemoveSubConn removes the SubConn from ClientConn.
      // The SubConn will be shutdown.
      RemoveSubConn(SubConn)

      // UpdateBalancerState is called by balancer to notify gRPC that some internal
      // state in balancer has changed.
      //
      // gRPC will update the connectivity state of the ClientConn, and will call pick
      // on the new picker to pick new SubConn.
      //
      // Deprecated: use UpdateState instead
      UpdateBalancerState(s connectivity.State, p Picker)

      // UpdateState notifies gRPC that the balancer's internal state has
      // changed.
      //
      // gRPC will update the connectivity state of the ClientConn, and will call pick
      // on the new picker to pick new SubConns.
      // 更新状态，让 V2Picker 选择连接
      UpdateState(State)

      // ResolveNow is called by balancer to notify gRPC to do a name resolving.
      // 触发服务发现
      ResolveNow(resolver.ResolveNowOptions)

      // Target returns the dial target for this ClientConn.
      //
      // Deprecated: Use the Target field in the BuildOptions instead.
      Target() string
  }

#+END_SRC
+  /V2Picker/ 接口，根据当前的连接列表，执行负载均衡策略选举一条连接发送rpc请求
#+BEGIN_SRC go
  // V2Picker is used by gRPC to pick a SubConn to send an RPC.
  // Balancer is expected to generate a new picker from its snapshot every time its
  // internal state has changed.
  //
  // The pickers used by gRPC can be updated by ClientConn.UpdateBalancerState().
  type V2Picker interface {
      // Pick returns the connection to use for this RPC and related information.
      //
      // Pick should not block.  If the balancer needs to do I/O or any blocking
      // or time-consuming work to service this call, it should return
      // ErrNoSubConnAvailable, and the Pick call will be repeated by gRPC when
      // the Picker is updated (using ClientConn.UpdateState).
      //
      // If an error is returned:
      //
      // - If the error is ErrNoSubConnAvailable, gRPC will block until a new
      //   Picker is provided by the balancer (using ClientConn.UpdateState).
      //
      // - If the error implements IsTransientFailure() bool, returning true,
      //   wait for ready RPCs will wait, but non-wait for ready RPCs will be
      //   terminated with this error's Error() string and status code
      //   Unavailable.
      //
      // - Any other errors terminate all RPCs with the code and message
      //   provided.  If the error is not a status error, it will be converted by
      //   gRPC to a status error with code Unknown.
      Pick(info PickInfo) (PickResult, error)
  }

#+END_SRC
** 逻辑
*** /passthroughResolver/ 解析 /Address/
#+BEGIN_SRC go
  func (ccr *ccResolverWrapper) UpdateState(s resolver.State) {
      if ccr.done.HasFired() {
          return
      }
      grpclog.Infof("ccResolverWrapper: sending update to cc: %v", s)
      if channelz.IsOn() {
          ccr.addChannelzTraceEvent(s)
      }
      ccr.curState = s
      ccr.poll(ccr.cc.updateResolverState(ccr.curState, nil))
  }
#+END_SRC

*** 更新 /ClientConn/ 的地址和 /ServiceConfig/
#+BEGIN_SRC go
#+END_SRC
*** lbWatcher
#+BEGIN_SRC go
  func (bwb *balancerWrapperBuilder) Build(cc balancer.ClientConn, opts balancer.BuildOptions) balancer.Balancer {
      bwb.b.Start(opts.Target.Endpoint, BalancerConfig{
          DialCreds: opts.DialCreds,
          Dialer:    opts.Dialer,
      })
      _, pickfirst := bwb.b.(*pickFirst)
      bw := &balancerWrapper{
          balancer:   bwb.b,
          pickfirst:  pickfirst,
          cc:         cc,
          targetAddr: opts.Target.Endpoint,
          startCh:    make(chan struct{}),
          conns:      make(map[resolver.Address]balancer.SubConn),
          connSt:     make(map[balancer.SubConn]*scState),
          csEvltr:    &balancer.ConnectivityStateEvaluator{},
          state:      connectivity.Idle,
      }
      // 初始状态是 Idle
      cc.UpdateState(balancer.State{ConnectivityState: connectivity.Idle, Picker: bw})
      // 这里会创建 SubConn 并建立实质的连接
      go bw.lbWatcher()
      return bw
  }

#+END_SRC
*** Connect的逻辑：
#+BEGIN_SRC go
  // connect starts creating a transport.
  // It does nothing if the ac is not IDLE.
  // TODO(bar) Move this to the addrConn section.
  func (ac *addrConn) connect() error {
      ac.mu.Lock()
      // 如果连接状态是 Shutdown 或 Idle 就直接返回
      if ac.state == connectivity.Shutdown {
          ac.mu.Unlock()
          return errConnClosing
      }
      if ac.state != connectivity.Idle {
          ac.mu.Unlock()
          return nil
      }
      // Update connectivity state within the lock to prevent subsequent or
      // concurrent calls from resetting the transport more than once.
      // 更新连接状态
      ac.updateConnectivityState(connectivity.Connecting, nil)
      ac.mu.Unlock()

      // Start a goroutine connecting to the server asynchronously.
      // 真正的去连接
      go ac.resetTransport()
      return nil
  }

#+END_SRC
* 数据结构
** 说明
综合切片与链表的优缺点，选择链表以求容量伸缩的时候有更好的内存分配性能。
#+BEGIN_SRC go
  type poolList struct {
      maxSize uint // 最大容量
      count   uint // 当前容量
      current uint // 当前指向
      head    *poolConn
  }
#+END_SRC
** 性能测试
+ 链表
[[./list.png]]
+ 切片
[[./slice.png]]
* 连接状态
** 说明
/grpc/ 连接的五个状态：
+ Idle: 由于缺少新的或待处理的RPC，通道甚至没有尝试创建连接的状态。在这种状态下可以创建新的RPC。任何在通道上启动RPC的尝试都会使该通道退出此状态以进行连接。
+ Connecting: 通道正在尝试建立连接，并且正在等待名称解析，TCP连接建立或TLS握手所涉及的步骤之一。创建时可以将其用作通道的初始状态。
+ Ready: 通道已通过TLS握手（或等效协议）和协议级（HTTP/2等）握手成功建立了连接，并且所有后续通信尝试均已成功（或未发生任何已知失败的挂起）。
+ TransientFailure: 发生了一些瞬时故障（例如TCP三次握手超时或套接字错误）。处于此状态的通道最终将切换到 /CONNECTING/ 状态，并尝试再次建立连接。
+ Shutdown: 此通道已开始关闭。任何新的RPC应该立即失败。待处理的RPC可以继续运行，直到应用程序将其取消为止。
** 检测
** 切换
这里需要注意一点的是 /grpc/ 的 /Dial/ 是异步的，调用完的连接状态是 /Connecting/ ，如果想达到 /Ready/ 状态的话需要 /WithBlock/ 。

状态切换流程：
#+BEGIN_SRC dot :file ./state.png :cmdline -Kdot -Tpng
  digraph demo {
      node[shape=box];
      resolver[label="passthrough.Build"];
      dd[label="ccResolverWrapper.UpdateState"];
      cc[label="ClientConn.updateResolverState"];
      ee[label="ccBalancerWrapper.newCCBalancerWrapper"];
      Build[label="balancerWrapperBuilder.Build"];
      lbWatcher[label="balancerWrapper.lbWatcher"];
      Connect[label="acBalancerWrapper.Connect"];
      connect[label="addrConn.connect"];
      resetTransport[label="addrConn.resetTra  nsport"];
      DialContext -> newCCResolverWrapper -> resolver;
      resolver -> dd -> cc -> ee -> Build;
      Build -> lbWatcher -> Connect -> connect -> resetTransport;
  }
#+END_SRC

#+RESULTS:
[[file:./state.png]]

最关键的就是 /func (ac *addrConn) resetTransport() {}/ 函数。大致的代码如下：
#+BEGIN_SRC go
  func (ac *addrConn) resetTransport() {
      for i := 0; ; i++ {
          /*
           .......
          */
          // We can potentially spend all the time trying the first address, and
          // if the server accepts the connection and then hangs, the following
          // addresses will never be tried.
          //
          // The spec doesn't mention what should be done for multiple addresses.
          // https://github.com/grpc/grpc/blob/master/doc/connection-backoff.md#proposed-backoff-algorithm
          connectDeadline := time.Now().Add(dialDuration)

          ac.updateConnectivityState(connectivity.Connecting, nil)
          ac.transport = nil
          ac.mu.Unlock()

          newTr, addr, reconnect, err := ac.tryAllAddrs(addrs, connectDeadline)
          if err != nil {
              // After exhausting all addresses, the addrConn enters
              // TRANSIENT_FAILURE.
              ac.mu.Lock()
              if ac.state == connectivity.Shutdown {
                  ac.mu.Unlock()
                  return
              }
              ac.updateConnectivityState(connectivity.TransientFailure, err)

              // Backoff.
              b := ac.resetBackoff
              ac.mu.Unlock()

              timer := time.NewTimer(backoffFor)
              select {
              case <-timer.C:
                  ac.mu.Lock()
                  ac.backoffIdx++
                  ac.mu.Unlock()
              case <-b:
                  timer.Stop()
              case <-ac.ctx.Done():
                  timer.Stop()
                  return
              }
              continue
          }

          ac.mu.Lock()
          if ac.state == connectivity.Shutdown {
              ac.mu.Unlock()
              newTr.Close()
              return
          }
          ac.curAddr = addr
          ac.transport = newTr
          ac.backoffIdx = 0

          hctx, hcancel := context.WithCancel(ac.ctx)
          ac.startHealthCheck(hctx)
          ac.mu.Unlock()

          // Block until the created transport is down. And when this happens,
          // we restart from the top of the addr list.
          <-reconnect.Done()
          hcancel()
          // restart connecting - the top of the loop will set state to
          // CONNECTING.  This is against the current connectivity semantics doc,
          // however it allows for graceful behavior for RPCs not yet dispatched
          // - unfortunate timing would otherwise lead to the RPC failing even
          // though the TRANSIENT_FAILURE state (called for by the doc) would be
          // instantaneous.
          //
          // Ideally we should transition to Idle here and block until there is
          // RPC activity that leads to the balancer requesting a reconnect of
          // the associated SubConn.
      }
  }

#+END_SRC

在该方法中会不断地去尝试创建连接，若成功则结束。否则不断地根据 /Backoff/ 算法的重试机制去尝试创建连接，直到成功为止。
** 关闭
#+BEGIN_SRC go
  // Close tears down the ClientConn and all underlying connections.
  func (cc *ClientConn) Close() error {
      // 最主要的操作，这里取消clientConn上下文，这会导致所有基础传输关闭。
      defer cc.cancel()

      cc.mu.Lock()
      if cc.conns == nil {
          cc.mu.Unlock()
          return ErrClientConnClosing
      }
      conns := cc.conns
      cc.conns = nil
      cc.csMgr.updateState(connectivity.Shutdown)

      rWrapper := cc.resolverWrapper
      cc.resolverWrapper = nil
      bWrapper := cc.balancerWrapper
      cc.balancerWrapper = nil
      cc.mu.Unlock()

      cc.blockingpicker.close()

      if rWrapper != nil {
          rWrapper.close()
      }
      if bWrapper != nil {
          bWrapper.close()
      }

      for ac := range conns {
          ac.tearDown(ErrClientConnClosing)
      }
      if channelz.IsOn() {
          ted := &channelz.TraceEventDesc{
              Desc:     "Channel Deleted",
              Severity: channelz.CtINFO,
          }
          if cc.dopts.channelzParentID != 0 {
              ted.Parent = &channelz.TraceEventDesc{
                  Desc:     fmt.Sprintf("Nested channel(id:%d) deleted", cc.channelzID),
                  Severity: channelz.CtINFO,
              }
          }
          channelz.AddTraceEvent(cc.channelzID, ted)
          // TraceEvent needs to be called before RemoveEntry, as TraceEvent may add trace reference to
          // the entity being deleted, and thus prevent it from being deleted right away.
          channelz.RemoveEntry(cc.channelzID)
      }
      return nil
  }

#+END_SRC
** 状态机逻辑
#+BEGIN_SRC go
  func (bw *balancerWrapper) HandleSubConnStateChange(sc balancer.SubConn, s connectivity.State) {
      bw.mu.Lock()
      defer bw.mu.Unlock()
      scSt, ok := bw.connSt[sc]
      if !ok {
          return
      }
      if s == connectivity.Idle {
          sc.Connect()
      }
      oldS := scSt.s
      scSt.s = s
      if oldS != connectivity.Ready && s == connectivity.Ready {
          scSt.down = bw.balancer.Up(scSt.addr)
      } else if oldS == connectivity.Ready && s != connectivity.Ready {
          if scSt.down != nil {
              scSt.down(errConnClosing)
          }
      }
      sa := bw.csEvltr.RecordTransition(oldS, s)
      if bw.state != sa {
          bw.state = sa
      }
      bw.cc.UpdateState(balancer.State{ConnectivityState: bw.state, Picker: bw})
      if s == connectivity.Shutdown {
          // Remove state for this sc.
          delete(bw.connSt, sc)
      }
  }

#+END_SRC
这里对各个状态作处理。
* 多路复用
** 简介
/HTTP2/ 的多路复用是指多个请求可以通过一个 /TCP/ 连接并发完成。
** 流（stream）
流是存在于客户端与服务端连接中的一个虚拟通道。流可以承载双向消息，每个流都有一个唯一的整数 /ID/ 。有以下几个特性：
+ 双向性：同一个流内，可同时发送和接受数据。
+ 有序性：流中被传输的数据就是二进制帧，帧在流上的被发送与被接收都是按照顺序进行的。
+ 并行性：流中的二进制帧都是被并行传输的，无需按顺序等待。
